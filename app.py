import streamlit as st
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage

# --- 1. ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ë¡œë“œ (ë¦¬íŠ¸ë¦¬ë²„ 4ê°œ, LLM 1ê°œ ë°˜í™˜) ---
@st.cache_resource
def load_resources():
    try:
        os.environ['GOOGLE_API_KEY'] = st.secrets["GOOGLE_API_KEY"]
    except Exception:
        st.error("Streamlit Secretsì— GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    qa_retriever = FAISS.load_local("./faiss_index_qa", embeddings, allow_dangerous_deserialization=True).as_retriever(search_kwargs={'k': 1})
    ict_retriever = FAISS.load_local("./faiss_index_ict", embeddings, allow_dangerous_deserialization=True).as_retriever(search_kwargs={'k': 5})
    tp_retriever = FAISS.load_local("./faiss_index_tp", embeddings, allow_dangerous_deserialization=True).as_retriever(search_kwargs={'k': 5})
    law_retriever = FAISS.load_local("./faiss_index_law", embeddings, allow_dangerous_deserialization=True).as_retriever(search_kwargs={'k': 5})
    
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.3)
    
    # ì´ 5ê°œì˜ ë³€ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    return qa_retriever, ict_retriever, tp_retriever, law_retriever, llm

# --- 2. ì²´ì¸ ë° í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
def setup_chains(llm):
    # (ì´ ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤)
    qa_gate_prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ Q&A ë¬¸ì„œë¥¼ ë³´ê³ , ì´ Q&Aê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ì™„ì „í•œ ë‹µë³€ì´ ë˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹µë³€ì€ 'yes' ë˜ëŠ” 'no'ë¡œë§Œ í•´ì£¼ì„¸ìš”.\n\n"
        "[ë¯¸ë¦¬ ì¤€ë¹„ëœ Q&A]:\n{qa_context}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]:\n{input}\n\n[Q&Aê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì´ ë©ë‹ˆê¹Œ? (yes/no)]:"
    )
    qa_gate_chain = qa_gate_prompt | llm | StrOutputParser()

    rewrite_prompt = ChatPromptTemplate.from_messages(...) # ì´ì „ ìµœì¢… í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼
    rewrite_chain = rewrite_prompt | llm | StrOutputParser()

    final_prompt = ChatPromptTemplate.from_messages(...) # ì´ì „ ìµœì¢… í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼
    final_chain = final_prompt | llm | StrOutputParser()
    
    return qa_gate_chain, rewrite_chain, final_chain

# --- 3. ë©”ì¸ ë¡œì§ ì‹¤í–‰ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë¶€ë¶„) ---
# í•¨ìˆ˜ê°€ íŠœí”Œ ëŒ€ì‹  ê°ê°ì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì§ì ‘ ë°›ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
def get_response(user_input, chat_history, qa_retriever, ict_retriever, tp_retriever, law_retriever, qa_gate_chain, rewrite_chain, final_chain):
    # Q&A DBì—ì„œ ë¨¼ì € ê²€ìƒ‰
    qa_docs = qa_retriever.invoke(user_input)
    
    if qa_docs:
        # Q&Aê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ë˜ëŠ”ì§€ LLMì´ íŒë‹¨
        is_answer_in_qa = qa_gate_chain.invoke({"qa_context": "\n".join([doc.page_content for doc in qa_docs]), "input": user_input})
        if "yes" in is_answer_in_qa.lower():
            # ë‹µì´ ë§ìœ¼ë©´, Q&Aì˜ ë‹µë³€ ë¶€ë¶„ì„ ë°”ë¡œ ë°˜í™˜í•˜ê³  ì¢…ë£Œ
            answer_part = qa_docs[0].page_content.split('A:')[1].strip()
            return f"âœ… [ëª¨ë²” ë‹µì•ˆ]:\n\n{answer_part}"

    # Q&Aì— ë‹µì´ ì—†ìœ¼ë©´, ê³„ì¸µì  RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    rewritten_question = rewrite_chain.invoke({"input": user_input, "chat_history": chat_history})
    ict_docs = ict_retriever.invoke(rewritten_question)
    tp_docs = tp_retriever.invoke(rewritten_question)
    law_docs = law_retriever.invoke(rewritten_question)
    
    # ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    final_answer = final_chain.invoke({
        "ict_context": "\n".join([doc.page_content for doc in ict_docs]),
        "tp_context": "\n".join([doc.page_content for doc in tp_docs]),
        "law_context": "\n".join([doc.page_content for doc in law_docs]),
        "chat_history": chat_history,
        "input": user_input
    })
    
    return final_answer

# --- Streamlit UI ì„¤ì • ---
st.set_page_config(page_title="ìµœì¢… ê·œì • ì§ˆì˜ì‘ë‹µ ì±—ë´‡", page_icon="ğŸ›ï¸")
st.title("ğŸ›ï¸ ìµœì¢… ê·œì • ì§ˆì˜ì‘ë‹µ ì±—ë´‡")
st.info("FAQ > ICTì§€ì¹¨ > TPê·œì • > ìƒìœ„ë²• ìˆœì„œë¡œ ë‹µë³€í•˜ë©°, ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤.")

try:
    # 5ê°œì˜ êµ¬ì„±ìš”ì†Œë¥¼ ê°ê°ì˜ ë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤.
    qa_retriever, ict_retriever, tp_retriever, law_retriever, llm = load_resources()
    qa_gate_chain, rewrite_chain, final_chain = setup_chains(llm)

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for message in st.session_state.chat_history:
        if isinstance(message, HumanMessage):
            with st.chat_message("user"): st.markdown(message.content)
        elif isinstance(message, AIMessage):
            with st.chat_message("assistant"): st.markdown(message.content)

    if prompt := st.chat_input("ê·œì •ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”..."):
        st.session_state.chat_history.append(HumanMessage(content=prompt))
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ë‹µë³€ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ìˆ˜ì •ëœ ë¶€ë¶„: get_response í•¨ìˆ˜ì— ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            answer = get_response(
                prompt,
                st.session_state.chat_history,
                qa_retriever,
                ict_retriever,
                tp_retriever,
                law_retriever,
                qa_gate_chain,
                rewrite_chain,
                final_chain
            )
            
            st.session_state.chat_history.append(AIMessage(content=answer))
            with st.chat_message("assistant"):
                st.markdown(answer)

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜ ìƒì„¸: {e}")
