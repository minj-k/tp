import streamlit as st
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document # Document íƒ€ì…ì„ ëª…ì‹œí•˜ê¸° ìœ„í•´ ì¶”ê°€

# --- ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì£¼ëŠ” í•¨ìˆ˜ ---
def format_docs(docs: list[Document]) -> str:
    """ê²€ìƒ‰ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤."""
    return "\n\n".join(doc.page_content for doc in docs)
# ---------------------------------------------------


# --- ì´ˆê¸° ì„¤ì • (ë¯¸ë¦¬ ë§Œë“¤ì–´ì§„ ë²¡í„°DB ë¡œë“œ) ---
@st.cache_resource
def load_rag_chain():
    try:
        os.environ['GOOGLE_API_KEY'] = st.secrets["GOOGLE_API_KEY"]
    except Exception as e:
        st.error("Streamlit Secretsì— GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.load_local("./faiss_index", embeddings, allow_dangerous_deserialization=True)
    st.sidebar.success("ì„ë² ë”© DB ë¡œë“œ ì™„ë£Œ!", icon="âœ…")

    retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
    prompt_template_str = """
    ë‹¹ì‹ ì€ 'ì „ë¶í…Œí¬ë…¸íŒŒí¬ ê·œì • ì•ˆë‚´ AI'ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ë‹µë³€ì€ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
    1. ë”±ë”±í•œ ê·œì • ë¬¸êµ¬ë³´ë‹¤ëŠ”, ë‚´ìš©ì„ ìš”ì•½í•˜ê³  í’€ì–´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    2. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” í•µì‹¬ ë‚´ìš©ì€ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    3. ì£¼ì–´ì§„ ë¬¸ë§¥ì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì „í˜€ ì—†ìœ¼ë©´, "ì£„ì†¡í•˜ì§€ë§Œ ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    4. ëª¨ë“  ë‹µë³€ì€ ì¹œì ˆí•œ ì–´ì¡°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

    [Context]
    {context}

    [Question]
    {question}

    Answer:
    """
    prompt = PromptTemplate.from_template(prompt_template_str)
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.2)

    # --- RAG ì²´ì¸ êµ¬ì„± (ìˆ˜ì •ëœ ë¶€ë¶„) ---
    rag_chain = (
        # retrieverì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ format_docs í•¨ìˆ˜ë¡œ ë„˜ê²¨ ë¬¸ìì—´ë¡œ ë³€í™˜
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    # ------------------------------------

    return rag_chain

# --- Streamlit UI êµ¬ì„± ë° ì‹¤í–‰ ---
try:
    rag_chain = load_rag_chain()
    st.set_page_config(page_title="ê·œì • ì§ˆì˜ì‘ë‹µ ì±—ë´‡", page_icon="ğŸ“š")
    st.title("ğŸ“š ê·œì • ì§ˆì˜ì‘ë‹µ ì±—ë´‡ (ë¯¸ë¦¬ í•™ìŠµëœ ë²„ì „)")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = rag_chain.invoke(prompt)
            with st.chat_message("assistant"):
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.info("API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€, faiss_index í´ë”ê°€ ì œëŒ€ë¡œ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
