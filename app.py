import streamlit as st
import os
import asyncio
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# --- ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì • ---
# Streamlit í™˜ê²½ì—ì„œ asyncioë¥¼ ì›í™œí•˜ê²Œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
try:
    loop = asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ï¿½ ì˜ˆì‚°ê´€ë¦¬ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
)

# --- ì œëª© ---
st.title("ğŸ’° ì˜ˆì‚°ê´€ë¦¬ ì±—ë´‡")
st.write("ë‚´ë¶€ ê·œì • ë¬¸ì„œ(PDF) ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")

# --- API í‚¤ ì„¤ì • ---
# Streamlitì˜ secrets ê´€ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©
try:
    google_api_key = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("Google API í‚¤ë¥¼ .streamlit/secrets.toml íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.info("secrets.toml íŒŒì¼ ì˜ˆì‹œ:\nGOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"")
    st.stop()

# --- ìƒìˆ˜ ì •ì˜ ---
DATA_FOLDER = "data"
FAISS_INDEX_PATH = "faiss_index_combined"

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

@st.cache_resource(show_spinner="ì§€ì‹ ë² ì´ìŠ¤(PDF)ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
def build_or_load_vector_store(data_folder, index_path):
    """
    ì§€ì •ëœ í´ë”ì˜ PDFë¥¼ ì½ì–´ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê±°ë‚˜, ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # 1. ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if os.path.exists(index_path):
        st.info("ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)
        vector_store = FAISS.load_local(
            folder_path=index_path,
            embeddings=embeddings,
            allow_dangerous_deserialization=True
        )
        return vector_store

    # 2. ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    st.info("ìƒˆë¡œìš´ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    all_documents = []
    pdf_files = [f for f in os.listdir(data_folder) if f.endswith('.pdf')]
    
    if not pdf_files:
        st.error(f"'{data_folder}' í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        st.stop()

    for pdf_file in pdf_files:
        loader = PyPDFLoader(os.path.join(data_folder, pdf_file))
        documents = loader.load()
        all_documents.extend(documents)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(all_documents)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)
    vector_store = FAISS.from_documents(texts, embeddings)
    vector_store.save_local(index_path)
    
    return vector_store

def create_conversational_rag_chain(vector_store):
    """
    VectorStoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™”í˜• RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=google_api_key, temperature=0.1)
    
    # Retriever ì„¤ì •: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë¥¼ 4ê°œë¡œ ì œí•œí•˜ì—¬ API ìš”ì²­ í¬ê¸°ë¥¼ ì¡°ì ˆ
    retriever = vector_store.as_retriever(search_kwargs={'k': 4})

    # 1. ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸
    # ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ëŒ€í™” ê¸°ë¡ì„ ì°¸ì¡°í•  í•„ìš”ê°€ ì—†ëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ í•˜ì§€ ë§ê³ , í•„ìš”í•œ ê²½ìš° ì§ˆë¬¸ì„ ì¬êµ¬ì„±ë§Œ í•˜ì„¸ìš”."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 2. ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸
    # **ì „ì²´ ëŒ€í™” ê¸°ë¡(chat_history)ì„ ì œì™¸í•˜ì—¬ API ìš”ì²­ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.**
    qa_system_prompt = """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    **ì¤‘ìš” ì§€ì¹¨:**
    1.  **ë‚´ìš© ê¸°ë°˜ ë‹µë³€:** ì•„ë˜ì— ì œê³µëœ <context> ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. <context>ë‚´ì— ì •í™•í•˜ê²Œ ì¼ì¹˜í•˜ì§€ëŠ” ì•Šì§€ë§Œ ë¹„ìŠ·í•œ ë‚´ìš©ì´ ìˆë‹¤ë©´ ìµœëŒ€í•œ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ì •ë‹µì— ê·¼ì ‘í•˜ê²Œ ëŒ€ë‹µì„ í•˜ì„¸ìš”.
    2.  **í‘œ(Table) ë°ì´í„° í™œìš©:** <context>ì— í‘œ í˜•ì‹ì˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´, ê·¸ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    3.  **ë‹µë³€ í˜•ì‹:** ë‹µë³€ì€ ìµœëŒ€í•œ ìƒì„¸í•˜ê³  ëª…í™•í•˜ê²Œ, ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    4.  **ì •ë³´ ë¶€ì¬ ì‹œ:** ë§Œì•½ <context> ì•ˆì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

    <context>
    {context}
    </context>
    """
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"), # MessagesPlaceholderë¥¼ ì œê±°í•˜ê³  ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì…ë ¥ë§Œ ì‚¬ìš©
        ]
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # 3. ë‘ ì²´ì¸ ê²°í•©
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain

# --- ë©”ì¸ ë¡œì§ ---

# 1. Vector Store ìƒì„± ë˜ëŠ” ë¡œë“œ
vector_store = build_or_load_vector_store(DATA_FOLDER, FAISS_INDEX_PATH)

# 2. ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±
conversational_rag_chain = create_conversational_rag_chain(vector_store)

# 3. ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ë‚´ë¶€ ê·œì • ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")]

# 4. ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.chat_history:
    if isinstance(message, AIMessage):
        with st.chat_message("AI"):
            st.write(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.write(message.content)

# 5. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    with st.chat_message("Human"):
        st.markdown(user_query)

    with st.chat_message("AI"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            # ì‘ë‹µì„ ë°›ê¸° ì „ì— ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë¡ì— ì¶”ê°€
            st.session_state.chat_history.append(HumanMessage(content=user_query))
            
            response = conversational_rag_chain.invoke(
                {"input": user_query, "chat_history": st.session_state.chat_history}
            )
            answer = response.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.write(answer)
            
            # ì‘ë‹µ ë°›ì€ í›„, AI ë‹µë³€ì„ ê¸°ë¡ì— ì¶”ê°€
            st.session_state.chat_history.append(AIMessage(content=answer))
