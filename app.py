import streamlit as st
import asyncio
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage

# --- ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì • ---
try:
    loop = asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ğŸ’° ì˜ˆì‚°ê´€ë¦¬ ì±—ë´‡",
    page_icon="ï¿½",
    layout="wide",
)

# --- ì œëª© ---
st.title("ğŸ’° ì˜ˆì‚°ê´€ë¦¬ ì±—ë´‡")
st.write("í†µí•©ëœ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

# --- API í‚¤ ì„¤ì • ---
try:
    google_api_key = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("Google API í‚¤ë¥¼ .streamlit/secrets.tomlì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# --- FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë° ì²´ì¸ ìƒì„± í•¨ìˆ˜ ---
@st.cache_resource(show_spinner="ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¡œë”©í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def load_retrieval_chain(index_path):
    """ì§€ì •ëœ ê²½ë¡œì˜ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  Retrieval ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # 1. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", 
        google_api_key=google_api_key
    )
    
    # 2. FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    vector_store = FAISS.load_local(
        folder_path=index_path, 
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )

    # 3. LLM ë° í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=google_api_key, temperature=0.1)
    
    # í‘œ ë°ì´í„° ì²˜ë¦¬ ì§€ì¹¨ì„ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ê°•í™”
    prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    **ì¤‘ìš” ì§€ì¹¨:**
    1.  **ë‚´ìš© ê¸°ë°˜ ë‹µë³€:** ì•„ë˜ì— ì œê³µëœ <context> ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ì¸¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    2.  **í‘œ(Table) ë°ì´í„° í™œìš©:** <context>ì— ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‘œê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, í‘œì˜ í–‰ê³¼ ì—´ ê´€ê³„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'Aì˜ BëŠ” ë¬´ì—‡ì¸ê°€?' ë¼ëŠ” ì§ˆë¬¸ì—ëŠ” í‘œì—ì„œ 'A' í–‰ê³¼ 'B' ì—´ì´ ë§Œë‚˜ëŠ” ê°’ì„ ì°¾ì•„ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    3.  **ë‹µë³€ í˜•ì‹:** ë‹µë³€ì€ ìµœëŒ€í•œ ìƒì„¸í•˜ê³  ëª…í™•í•˜ê²Œ, ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    4.  **ì •ë³´ ë¶€ì¬ ì‹œ:** ë§Œì•½ <context> ì•ˆì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

    <context>
    {context}
    </context>

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {input}

    [ë‹µë³€]
    """)
    
    # 4. Retrieval ì²´ì¸ ìƒì„±
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = vector_store.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    
    return retrieval_chain

# --- ë©”ì¸ ë¡œì§ ---

# í•­ìƒ ë‹¨ì¼ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œ
FAISS_INDEX_PATH = "faiss_index_combined"
retrieval_chain = load_retrieval_chain(FAISS_INDEX_PATH)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì‹œë©´ ë‹µë³€í•´ ë“œë¦´ê²Œìš”."),
    ]

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.chat_history:
    if isinstance(message, AIMessage):
        with st.chat_message("AI"):
            st.write(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.write(message.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€ ë° í™”ë©´ì— í‘œì‹œ
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    with st.chat_message("Human"):
        st.markdown(user_query)

    # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("AI"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            response = retrieval_chain.invoke({
                "chat_history": st.session_state.chat_history,
                "input": user_query
            })
            
            if "answer" in response:
                st.write(response["answer"])
                st.session_state.chat_history.append(AIMessage(content=response["answer"]))
            else:
                st.error("ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
