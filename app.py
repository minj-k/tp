# app.py

import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
import os

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ğŸ’° ì˜ˆì‚°ê´€ë¦¬ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
)

# --- ì œëª© ---
st.title("ğŸ’° ì˜ˆì‚°ê´€ë¦¬ ì±—ë´‡")
st.write("ì§ˆë¬¸ ë‚´ìš©ì— ë§ì¶° ê°€ì¥ ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ ë‹µë³€í•©ë‹ˆë‹¤.")

# --- API í‚¤ ì„¤ì • ---
try:
    google_api_key = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("Google API í‚¤ë¥¼ .streamlit/secrets.tomlì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# --- ì§€ì‹ ë² ì´ìŠ¤ ì„¤ëª… ì •ì˜ ---
# ê° FAISS ì¸ë±ìŠ¤ í´ë”ê°€ ì–´ë–¤ ë‚´ìš©ì„ ë‹¤ë£¨ëŠ”ì§€ LLMì—ê²Œ ì•Œë ¤ì£¼ê¸° ìœ„í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
# í´ë” ì´ë¦„ì€ ì‹¤ì œ í”„ë¡œì íŠ¸ì˜ í´ë” ì´ë¦„ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
KNOWLEDGE_BASE_DESCRIPTIONS = {
    "faiss_index_ict": "ì •ë³´í†µì‹ ê¸°ìˆ (ICT)ê³¼ ê´€ë ¨ëœ ìµœì‹  ê¸°ìˆ , íŠ¸ë Œë“œ, ìš©ì–´ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.",
    "faiss_index_law": "ë²•ë¥ , ê·œì œ, íŒë¡€ ë“± ë²•ê³¼ ê´€ë ¨ëœ ì „ë¬¸ì ì¸ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "faiss_index_qa": "ì¼ë°˜ì ì¸ ì§ˆë¬¸ê³¼ ë‹µë³€(Q&A) í˜•ì‹ì˜ ë§¤ë‰´ì–¼ì…ë‹ˆë‹¤.",
    "faiss_index_tp": "í…Œí¬ë…¸íŒŒí¬(TP)ì˜ ê·œì • ìë£Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
}

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

def get_best_knowledge_base(user_query):
    """ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ í´ë” ì´ë¦„ì„ ê²°ì •í•˜ëŠ” ë¼ìš°í„° í•¨ìˆ˜."""
    llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=google_api_key, temperature=0)
    
    # ì§€ì‹ ë² ì´ìŠ¤ ì„¤ëª…ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    descriptions_text = "\n".join([f"- {name}: {desc}" for name, desc in KNOWLEDGE_BASE_DESCRIPTIONS.items()])
    
    prompt = ChatPromptTemplate.from_template(f"""
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ê³¼ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì´ë¦„(í´ë”ëª…) ë‹¨ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

    [ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡]
    {descriptions_text}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {{question}}

    [ê°€ì¥ ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ ì´ë¦„]
    """)
    
    routing_chain = prompt | llm
    
    # .contentë¥¼ í†µí•´ ê²°ê³¼ ë¬¸ìì—´ë§Œ ì¶”ì¶œí•˜ê³ , ê³µë°± ì œê±°
    result = routing_chain.invoke({"question": user_query}).content.strip()
    
    # ìœ íš¨í•œ í´ë” ì´ë¦„ì¸ì§€ í™•ì¸
    if result in KNOWLEDGE_BASE_DESCRIPTIONS:
        return result
    else:
        # LLMì´ ì˜ˆìƒ ì™¸ì˜ ë‹µë³€ì„ í•  ê²½ìš°, ê¸°ë³¸ê°’ìœ¼ë¡œ fallback (ì˜ˆ: 'faiss_index_qa')
        return "faiss_index_qa" 

@st.cache_resource(show_spinner="ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¡œë”©í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def load_retrieval_chain(index_name):
    """ì„ íƒëœ ì´ë¦„ì˜ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  Retrieval ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)
    vector_store = FAISS.load_local(
        folder_path=index_name, 
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )

    llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=google_api_key, temperature=0.3)
    
    prompt = ChatPromptTemplate.from_template("""
    ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ë‹¤ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.

    <context>
    {context}
    </context>

    Question: {input}
    """)
    
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = vector_store.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    
    return retrieval_chain

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì‹œë©´ ê´€ë ¨ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì°¾ì•„ ë‹µë³€í•´ ë“œë¦´ê²Œìš”."),
    ]

# --- ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
for message in st.session_state.chat_history:
    if isinstance(message, AIMessage):
        with st.chat_message("AI"):
            st.write(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.write(message.content)

# --- ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ---
if user_query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€ ë° í™”ë©´ì— í‘œì‹œ
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    with st.chat_message("Human"):
        st.markdown(user_query)

    # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("AI"):
        with st.spinner("ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì°¾ëŠ” ì¤‘..."):
            # 1ë‹¨ê³„: ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì§€ì‹ ë² ì´ìŠ¤ ì„ íƒ (ë¼ìš°íŒ…)
            selected_index = get_best_knowledge_base(user_query)
            st.info(f"'{KNOWLEDGE_BASE_DESCRIPTIONS[selected_index]}' ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.")

        with st.spinner(f"'{selected_index}'ì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            # 2ë‹¨ê³„: ì„ íƒëœ ì§€ì‹ ë² ì´ìŠ¤ë¡œ RAG ì²´ì¸ ë¡œë“œ ë° ë‹µë³€ ìƒì„±
            retrieval_chain = load_retrieval_chain(selected_index)
            response = retrieval_chain.invoke({
                "chat_history": st.session_state.chat_history,
                "input": user_query
            })
            
            if "answer" in response:
                st.write(response["answer"])
                st.session_state.chat_history.append(AIMessage(content=response["answer"]))
            else:
                st.error("ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
